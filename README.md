# Customer Service RAG Application

A structured Retrieval-Augmented Generation (RAG) application implementing a corrective retrieval pipeline to improve answer reliability and reduce hallucinations.
This project demonstrates practical RAG system design, vector database integration, query rewriting, and relevance-based document filtering.

## Overview

This application implements a corrective RAG workflow for customer service scenarios.

It supports:

- Context-aware multi-turn conversations
- Relevance grading and document filtering
- Query rewriting when retrieval quality is insufficient
- Source citation with confidence scoring
- Persistent vector storage with Qdrant
- The focus of this project is system design and retrieval quality optimization rather than UI complexity.

## Architecture

The system is organized into modular services to separate retrieval, ranking, generation, and storage.

```
User Input â†’ Streamlit Interface â†’ Chat Service
                â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                â†“         â†“          â†“
    Retriever      Reranker  Query Rewriter  Generator
        â†“                â†“         â†“          â†“
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
                  OpenAI API + Qdrant
                        â†“
                 Final Response
```

## Technical Stack

- Python 3.8+
- OpenAI GPT models
- OpenAI Embeddings API
- Qdrant (Vector Database)
- Streamlit (UI layer)
- Docker / Docker Compose

## Engineering Highlights

- Corrective RAG pipeline with query rewriting
- Modular service-based architecture
- Separation of retrieval, ranking, and generation logic
- Configurable retrieval parameters
- Persistent vector storage
- Confidence-aware response generation
- Multi-turn conversation context handling

## Quick Start

### Setup

1. **Clone the repositor**
```bash
git clone <your-repo-url>
cd customer-service-rag
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY
```

5. **Start Qdrant**
```bash
docker-compose up -d
```

6. **Initialize vector database**
```bash
python scripts/init_vector_db.py
```

7. **Load knowledge base**
```bash
python scripts/load_knowledge_base.py
```

8. **Run the application**
```bash
streamlit run app.py
```

The application will open at `http://localhost:8501`

## Project Structure

```
customer-service-rag/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ chat_service.py
â”œâ”€â”€ knowledge_service.py
â”œâ”€â”€ retriever.py
â”œâ”€â”€ reranker.py
â”œâ”€â”€ query_rewriter.py
â”œâ”€â”€ generator.py
â”œâ”€â”€ vector_store.py
â”œâ”€â”€ llm_client.py
â”œâ”€â”€ config.py
â”œâ”€â”€ document_loader.py
â”œâ”€â”€ logger_config.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_vector_db.py
â”‚   â””â”€â”€ load_knowledge_base.py
â”œâ”€â”€ data/
â””â”€â”€ requirements.txt
```

## Design Considerations

- Corrective retrieval to reduce hallucination risk
- Query rewriting to improve recall
- Separation of retrieval and generation for maintainability
- Configurable hyperparameters for experimentation
- Persistent vector storage for production-like behavior

## ğŸ”„ Corrective RAG Workflow / çŸ«æ­£å¼RAGå·¥ä½œæµ

```
1. User Query â†’ Initial Retrieval
2. Relevance Grading â†’ Filter Documents
3. Decision: Enough Relevant Docs?
   â”œâ”€ YES â†’ Generate Answer
   â””â”€ NO  â†’ Rewrite Query â†’ Re-retrieve â†’ Generate Answer
```

